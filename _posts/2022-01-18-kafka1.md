---
layout: post
title:  "Using kafka locally"
date:   2023-01-18 23:17:05 +0530
categories: [java, kafka, draft]
---

This post is to document basic steps required to start, write to and read from kafka locally.

### Starting kafka server locally

#### Download
- First we need to download kafka binaries from [kafka downloads](https://kafka.apache.org/downloads).
- Downloaded file will be named something like `kafka_2.13-3.X.X.tgz`. Assuming downloaded file is present in `~/Downloads`.
- Untar this file using command
```
tar -xvzf ~/Downloads/kafka_2.13-3.X.X.tgz ~/kafka
```
- Move directory to make it easier to find
```
mv ~/Downloads/kafka_2.13-3.X.X ~/kafka
```

#### Start zookeeper
There will be many scripts present in `~/kafka/bin`. Zookeeper can be started using `zookeeper-server-start.sh` script by passing in server configuration

```bash
cd ~/kafka
bin/zookeeper-server-start.sh config/zookeeper.properties
```
zookeeper service will be running locally on port `2181`.

#### Start kafka
Kafka server can be started using `kafka-server-start.sh` script by passing in kafka server configuration

```bash
cd ~/kafka
bin/kafka-server-start.sh config/server.properties
```
kafka service will be running locally on port `9092`.

****

### Writing to kafka
For writing to kafka, we will use [KafkaProducer](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/producer/KafkaProducer.html).

#### setup producer
First we need to setup properties, so that kafka producer knows
- which kafka server to connect to, and
- how to serialize key and value before writing

```java
var props = new Properties();
props.put("bootstrap.servers", "http://kafka.local:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

var producer = new KafkaProducer(props);
```
Using these properties `KafkaProducer` will connect to earlier started kafka server and write both key/value as string.


#### writing using producer
To write data, we need to specify topicName, key & value.
- topicName is name of topic where this data will be written. If topic does not exist on server it will be created when first data point is written.
- key is used to distribute data across partitions of a topic. This ensures data for same keys is written to same partition so that ordering is maintained.
- value is the actual data that we want to pass to downstream service using kafka

```java
var record = new ProducerRecord<K, V>(topicName, key, value);
producer.send(record);
producer.flush();               // This is to ensure data is written to broker immediately
```

***

### Reading from kafka
For reading from kafka, we will use [KafkaConsumer](https://kafka.apache.org/25/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html).

#### setup consumer
First we need to setup properties, so that kafka consumer knows
- which kafka server to connect to, and
- how to deserialize key and value before writing

```java
var props = new Properties();
props.put("bootstrap.servers", "http://kafka.local:9092");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

var consumer = new KafkaConsumer(properties);
```
Using these properties `KafkaConsumer` will connect to earlier started kafka server and read both key/value as string.


#### reading using consumer
- To read data, we need to first subscribe to topic.
```java
consumer.subscribe(List.of(topicName));
```
- next we have to poll for record from kafka server
```java
var records = consumer.poll(Duration.ofMillis(100));
```
Consumer will read records is they are available otherwise it will wait for 100ms before returning empty records.
